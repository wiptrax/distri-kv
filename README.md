# ğŸ—ƒï¸ Distributed Key-Value Store in Go

A lightweight distributed key-value store implemented in Go with master-slave replication, consistent sharding, and client redirection when hash-based routing detects a mismatch. 

---

## ğŸš€ Features

- âœ… **Written in Go** â€” fast and simple concurrency model using goroutines.
- ğŸ”‘ **Consistent Hashing** â€” deterministic shard placement based on key hashes.
- ğŸ§­ **Redirection Logic** â€” automatic to the correct shard if the request hits the wrong node.
- ğŸ§± **Master-Slave Replication** â€” every shard has a read replica.
- ğŸ—‚ï¸ **Pluggable Storage** â€” stores data on-disk via embedded database per shard.


---

## âš™ï¸ Getting Started

### ğŸ”§ Prerequisites

- Go 1.20+
- Curl/Postman for testing

---

### ğŸ› ï¸ Build

```bash
make build
```

â–¶ï¸ Run Servers

```bash
make run-multi
```

This starts multiple shard nodes (Delhi, Mumbai, Hyderabad, Chennai), each with a master and replica.

ğŸ“¥ Set a Key

```bash
curl "http://127.0.0.2:8080/set?key=mykey&value=myvalue"
```

If the key doesn't belong to this shard based on consistent hashing, the server will respond with a redirect to the correct shard.

ğŸ“¤ Get a Key

```bash
curl "http://127.0.0.2:8080/get?key=mykey"
```

ğŸ§ª Load Testing

```bash
make load-test
```

This sends 1000 randomized key-value pairs to multiple shards to simulate distributed load.

ğŸ§¹ Cleanup

```bash
make remove-all
```

Removes all .db files for a clean restart.


ğŸ“ Folder Structure

```
.
â”œâ”€â”€ cmd/                # CLI and startup logic
â”œâ”€â”€ config/             # TOML config files for sharding
â”œâ”€â”€ db/                 # (Optional) database files
â”œâ”€â”€ replication/        # Master-slave logic
â”œâ”€â”€ web/                # HTTP server handlers
â”œâ”€â”€ main.go             # Entry point
â”œâ”€â”€ Makefile            # Build and orchestration
â”œâ”€â”€ sharding.toml       # Shard mapping
```

ğŸ§  Concepts

- Consistent Hashing ensures uniform key distribution across shards.
- Redirection makes any node entry-point possible â€” the router will redirect if needed.
- Replication provides read availability via replicas (eventually consistent).

ğŸŠ Example

![example](https://github.com/wiptrax/distri-kv/blob/main/example.png)

ğŸ›£ï¸ Future Improvements

- Leader election for fault tolerance
- gRPC API for more efficient communication
- Persistent WAL for better durability
- Dynamic shard rebalancing

ğŸ“ License

MIT â€” feel free to use, modify, and build on top of this project.
